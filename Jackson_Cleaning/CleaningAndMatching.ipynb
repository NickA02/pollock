{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import os\n",
    "from constants import *\n",
    "import string\n",
    "import json\n",
    "from difflib import SequenceMatcher\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get printable characters\n",
    "printable = set(string.printable)\n",
    "\n",
    "metadataDict = {}\n",
    "# iterate through directory where articles are stored\n",
    "with open(metadataPath, 'r') as file:\n",
    "    for i in range(10):\n",
    "        line = next(file).strip()\n",
    "        if json.loads(line)['id'] not in metadataDict:\n",
    "            abstract = json.loads(line)['abstract']\n",
    "            metadataDict[json.loads(line)['id']] = ''.join(filter(lambda x: x in printable, abstract))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0704.0003v2.txt\n",
      "0704.0007v1.txt\n",
      "0704.0008v1.txt\n",
      "0704.0009v1.txt\n",
      "0704.0010v1.txt\n",
      "0704.0012v1.txt\n"
     ]
    }
   ],
   "source": [
    "articleDict = {}\n",
    "# loop through each file in folder that contains articles\n",
    "for fileName in os.listdir(articlesPath):\n",
    "    print(fileName)\n",
    "    with open(f'{articlesPath}/{fileName}', 'r', encoding='cp1252', errors='ignore') as file:\n",
    "        path = os.path.basename(fileName)\n",
    "        dictkey = path[0:9]\n",
    "        articleDict[dictkey] = \" \".join(file.readlines())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID: 0704.0003 \n",
      "matched abstract: \n",
      "   The evolution of Earth-Moon system is described by the dark matter field fluid model with a non-Newtonian approach proposed in the Meeting of Division of Particle and Field 2004, American Physical Society. The current behavior of the Earth-Moon system agrees with this model very well and the general pattern of the evolution of the Moon-Earth system described by this model agrees with geological and fossil evidence. The closest distance of the Moon to Earth was about 259000 km at 4.5 billion years ago, which is far beyond the Roche’s limit. The result suggests that the tidal friction may not be the primary cause for the evolution of the Earth-Moon system. The average dark matter field fluid constant derived from Earth-Moon system data is 4.39 × 10-22 s-1m-1. This model predicts that the Mars’s rotation is also slowing with the angular acceleration rate about -4.38 × 10-22 \n",
      "---- \n",
      "original abstract from metadata: \n",
      "   The evolution of Earth-Moon system is described by the dark matter fieldfluid model proposed in the Meeting of Division of Particle and Field 2004,American Physical Society. The current behavior of the Earth-Moon system agreeswith this model very well and the general pattern of the evolution of theMoon-Earth system described by this model agrees with geological and fossilevidence. The closest distance of the Moon to Earth was about 259000 km at 4.5billion years ago, which is far beyond the Roche's limit. The result suggeststhat the tidal friction may not be the primary cause for the evolution of theEarth-Moon system. The average dark matter field fluid constant derived fromEarth-Moon system data is 4.39 x 10^(-22) s^(-1)m^(-1). This model predictsthat the Mars's rotation is also slowing with the angular acceleration rateabout -4.38 x 10^(-22) rad s^(-2). \n",
      "---- \n",
      "similarity(of the strings with no spaces): 0.9592391304347826 \n",
      "\n",
      "Length of matched abstract: 885\n",
      "Length of original abstract: 868\n",
      "------------------------------------------------------\n",
      "Article ID: 0704.0007 \n",
      "matched abstract: \n",
      "  A rather non-standard quantum representation of the canoni cal commutation relations of quan- tum mechanics systems, known as the polymer representation has gained some attention in recent years, due to its possible relation with Planck scale physic s. In particular, this approach has been followed in a symmetric sector of loop quantum gravity known as loop quantum cosmology (LQC). Here we explore diï¬€erent aspects of the relation between the ordinary SchrÂ¨ odinger theory and the polymer description. The paper has two parts. In the ï¬rst one , we derive the polymer quantum mechanics starting from the ordinary SchrÂ¨ odinger theory a nd show that the polymer description arises as an appropriate limit. In the second part we conside r the continuum limit of this theory, namely, the reverse process in which one starts from the disc rete theory and tries to recover back the ordinary SchrÂ¨ odinger quantum mechanics. We consider s everal examples of interest, including the harmonic oscillator, the free particle and a simple cosm ological model. \n",
      "---- \n",
      "original abstract from metadata: \n",
      "   A rather non-standard quantum representation of the canonical commutationrelations of quantum mechanics systems, known as the polymer representation hasgained some attention in recent years, due to its possible relation with Planckscale physics. In particular, this approach has been followed in a symmetricsector of loop quantum gravity known as loop quantum cosmology. Here we exploredifferent aspects of the relation between the ordinary Schroedinger theory andthe polymer description. The paper has two parts. In the first one, we derivethe polymer quantum mechanics starting from the ordinary Schroedinger theoryand show that the polymer description arises as an appropriate limit. In thesecond part we consider the continuum limit of this theory, namely, the reverseprocess in which one starts from the discrete theory and tries to recover backthe ordinary Schroedinger quantum mechanics. We consider several examples ofinterest, including the harmonic oscillator, the free particle and a simplecosmological model. \n",
      "---- \n",
      "similarity(of the strings with no spaces): 0.9830124575311439 \n",
      "\n",
      "Length of matched abstract: 1057\n",
      "Length of original abstract: 1022\n",
      "------------------------------------------------------\n",
      "Article ID: 0704.0008 \n",
      "matched abstract: \n",
      "  A general formulation was developed to represent material m odels for applications in dynamic loading. Numerical methods were devised to calcu late response to shock and ramp compression, and ramp decompression, generalizin g previous solutions for scalar equations of state. The numerical methods were fo und to be ï¬‚exible and robust, and matched analytic results to a high accuracy. The basic ramp and shock solution methods were coupled to solve for composite deform ation paths, such as shock-induced impacts, and shock interactions with a plana r interface between dif- ferent materials. These calculations capture much of the ph ysics of typical material dynamics experiments, without requiring spatially-resol ving simulations. Example calculations were made of loading histories in Be and Mo, ill ustrating the eï¬€ects of plastic work on the temperatures induced in quasi-isentr opic and shock-release experiments. Key words \n",
      "---- \n",
      "original abstract from metadata: \n",
      "   A general formulation was developed to represent material models forapplications in dynamic loading. Numerical methods were devised to calculateresponse to shock and ramp compression, and ramp decompression, generalizingprevious solutions for scalar equations of state. The numerical methods werefound to be flexible and robust, and matched analytic results to a highaccuracy. The basic ramp and shock solution methods were coupled to solve forcomposite deformation paths, such as shock-induced impacts, and shockinteractions with a planar interface between different materials. Thesecalculations capture much of the physics of typical material dynamicsexperiments, without requiring spatially-resolving simulations. Examplecalculations were made of loading histories in metals, illustrating the effectsof plastic work on the temperatures induced in quasi-isentropic andshock-release experiments, and the effect of a phase transition. \n",
      "---- \n",
      "similarity(of the strings with no spaces): 0.9609423434593924 \n",
      "\n",
      "Length of matched abstract: 936\n",
      "Length of original abstract: 936\n",
      "------------------------------------------------------\n",
      "Article ID: 0704.0009 \n",
      "matched abstract: \n",
      "  We discuss the results from the combined IRAC and MIPS c2d Spi tzer Legacy observations of the Serpens star-forming region. In partic ular we present a set of criteria for isolating bona ï¬de young stellar objects, YSOâ€™ s, from the extensive background contamination by extra-galactic objects. We th en discuss the prop- erties of the resulting high conï¬dence set of YSOâ€™s. We ï¬nd 23 5 such objects in the 0.85 deg2ï¬eld that was covered with both IRAC and MIPS. An additional set of 51 lower conï¬dence YSOâ€™s outside this area is identiï¬e d from the MIPS data combined with 2MASS photometry. To understand the prop erties of the circumstellar material that produces the observed infrare d emission, we describe two sets of results, the use of color-color diagrams to compa re our observed source properties with those of theoretical models for star/disk/ envelope systems and our own modeling of the subset of our objects that appear to be well represented by a stellar photosphere plus circumstellar disk. These obj ects exhibit a very wide range of disk properties, from many that can be ï¬t with ac tively accreting disks to some with both passive disks and even possibly debri s disks. We ï¬nd that the luminosity function of YSOâ€™s in Serpens extends dow n to at least a fewÃ—10âˆ’3LâŠ™or lower for an assumed distance of 260 pc. The lower limit may be set by our inability to distinguish YSOâ€™s from extra-gala ctic sources more than by the lack of YSOâ€™s at very low luminosities. We ï¬nd no ev idence for 1Astronomy Department, University of Texas at Austin, 1 Univ ersity Station C1400, Austin, TX 78712- 0259; pmh@astro.as.utexas.edu, nje@astro.as.utexas.ed u 2Research \n",
      "---- \n",
      "original abstract from metadata: \n",
      "   We discuss the results from the combined IRAC and MIPS c2d Spitzer Legacyobservations of the Serpens star-forming region. In particular we present a setof criteria for isolating bona fide young stellar objects, YSO's, from theextensive background contamination by extra-galactic objects. We then discussthe properties of the resulting high confidence set of YSO's. We find 235 suchobjects in the 0.85 deg^2 field that was covered with both IRAC and MIPS. Anadditional set of 51 lower confidence YSO's outside this area is identifiedfrom the MIPS data combined with 2MASS photometry. We describe two sets ofresults, color-color diagrams to compare our observed source properties withthose of theoretical models for star/disk/envelope systems and our own modelingof the subset of our objects that appear to be star+disks. These objectsexhibit a very wide range of disk properties, from many that can be fit withactively accreting disks to some with both passive disks and even possiblydebris disks. We find that the luminosity function of YSO's in Serpens extendsdown to at least a few x .001 Lsun or lower for an assumed distance of 260 pc.The lower limit may be set by our inability to distinguish YSO's fromextra-galactic sources more than by the lack of YSO's at very low luminosities.A spatial clustering analysis shows that the nominally less-evolved YSO's aremore highly clustered than the later stages and that the backgroundextra-galactic population can be fit by the same two-point correlation functionas seen in other extra-galactic studies. We also present a table of matchesbetween several previous infrared and X-ray studies of the Serpens YSOpopulation and our Spitzer data set. \n",
      "---- \n",
      "similarity(of the strings with no spaces): 0.6886858749121574 \n",
      "\n",
      "Length of matched abstract: 1693\n",
      "Length of original abstract: 1693\n",
      "------------------------------------------------------\n",
      "Article ID: 0704.0010 \n",
      "matched abstract: \n",
      "  Partial cubes are isometric subgraphs of hypercubes. Struc tures on a graph deï¬ned by means of semicubes, and DjokoviÂ´ câ€™s and Wink lerâ€™s rela- tions play an important role in the theory of partial cubes. T hese struc- tures are employed in the paper to characterize bipartite gr aphs and par- tial cubes of arbitrary dimension. New characterizations a re established and new proofs of some known results are given. The operations of Cartesian product and pasting, and expans ion and contraction processes are utilized in the paper to construc t new partial cubes from old ones. In particular, the isometric and lattic e dimensions of ï¬nite partial cubes obtained by means of these operations ar e calculated. \n",
      "---- \n",
      "original abstract from metadata: \n",
      "   Partial cubes are isometric subgraphs of hypercubes. Structures on a graphdefined by means of semicubes, and Djokovi\\'{c}'s and Winkler's relations playan important role in the theory of partial cubes. These structures are employedin the paper to characterize bipartite graphs and partial cubes of arbitrarydimension. New characterizations are established and new proofs of some knownresults are given.  The operations of Cartesian product and pasting, and expansion andcontraction processes are utilized in the paper to construct new partial cubesfrom old ones. In particular, the isometric and lattice dimensions of finitepartial cubes obtained by means of these operations are calculated. \n",
      "---- \n",
      "similarity(of the strings with no spaces): 0.9689857502095558 \n",
      "\n",
      "Length of matched abstract: 717\n",
      "Length of original abstract: 693\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def similar(a, b):\n",
    "    # this method calculates the similarity score between two strings\n",
    "    # Im removing spaces here to avoid spacing issues messing up similarity scores\n",
    "    return SequenceMatcher(None, a.replace(\" \", \"\"), b.replace(\" \", \"\")).ratio()\n",
    "\n",
    "def printMatch(metadataAbstract, maxMatchinString, similarity, articleID):\n",
    "    print(f'Article ID: {articleID} ')\n",
    "    print(f'matched abstract: \\n {maxMatchingString} \\n---- \\noriginal abstract from metadata: \\n {metadataAbstract} \\n---- \\nsimilarity(of the strings with no spaces): {similarity} \\n')\n",
    "    print(f'Length of matched abstract: {len(maxMatchingString)}')\n",
    "    print(f'Length of original abstract: {len(metadataAbstract)}')\n",
    "    print('------------------------------------------------------')\n",
    "\n",
    "for articleID in articleDict:\n",
    "    articleText = articleDict[articleID].replace('\\n', \"\")\n",
    "    if articleID in metadataDict:\n",
    "        metadataAbstract = metadataDict[articleID].replace('\\n', \"\")\n",
    "    else:\n",
    "        continue    \n",
    "    i = 0\n",
    "    maxMatchingString = ''\n",
    "    maxMatch = 0\n",
    "    abstractFound = False\n",
    "    while i  < len(articleText):\n",
    "        # find the abstract in the article, and move index to that location\n",
    "        if articleText[i: i + 8].lower() == 'abstract':\n",
    "            abstractFound = True\n",
    "            i += 8\n",
    "        elif not abstractFound:\n",
    "            i += 1\n",
    "            continue\n",
    "        lengthMarginCounter = 0\n",
    "        broken = False\n",
    "        lengthMargin = 50\n",
    "        # calculate similarities of strings with a margin of lengthMargin, meaning its length can be at most lengthMargin characters greater than the original\n",
    "        # this heavily increases computation time but is able to match up to lengthMargin more characters \n",
    "        # 6 articles: value of 1 -> 1 second, value of 50 -> 25 seconds\n",
    "        while lengthMarginCounter < lengthMargin:\n",
    "            candidateAbstract = articleText[i:i + lengthMarginCounter + len(metadataAbstract)]\n",
    "            similarity = similar(metadataAbstract, candidateAbstract)\n",
    "            # if the similarity of the current candidate string is higher\n",
    "            if similarity > maxMatch:\n",
    "                maxMatchingString = candidateAbstract\n",
    "                maxMatch = similarity\n",
    "            # otherwise, if the similarity < .75 and the similarity score is not increasing, break \n",
    "            # (assuming that similarity score will increase as the abstract should be at beginning)\n",
    "            elif similarity < .75:\n",
    "                broken = True\n",
    "                break\n",
    "            lengthMarginCounter += 1\n",
    "        if broken:\n",
    "            break\n",
    "        i += 1\n",
    "    printMatch(metadataAbstract, maxMatchingString, maxMatch, articleID)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "93c5346a6a3d07125b79d64fc117728ba646bd89aaa0c0db68034b154e209009"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
